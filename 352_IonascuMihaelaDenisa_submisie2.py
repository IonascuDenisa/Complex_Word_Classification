# -*- coding: utf-8 -*-
"""Proiect_IA_submisie_12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wU7BYbyQgYMT3KLZJpDRjrdswAHvVBfm

Instalari si importuri necesare
"""

import nltk
nltk.download('punkt')
from sklearn.naive_bayes import GaussianNB
import regex as re
from collections import Counter    
!pip install wordfreq
from wordfreq import word_frequency
from nltk.corpus import wordnet
nltk.download('wordnet')
import string
nltk.download('averaged_perceptron_tagger')

from sklearn.metrics import balanced_accuracy_score
!pip install pyphen
import pyphen
import numpy as np
import pandas as pd
from nltk.corpus import wordnet
from dale_chall import DALE_CHALL
from nltk.tokenize import word_tokenize
from wordfreq import zipf_frequency

"""Citirea datelor in dataframe"""

path="D:\\materii\\an3\\ia\\PROIECT\\date\\"
path2="D:\\materii\\an3\\ia\\PROIECT\\test_folds.xlsx"
dtypes = {"sentence": "string", "token": "string", "complexity": "float64"}
train = pd.read_excel('train.xlsx', dtype=dtypes, keep_default_na=False)
test = pd.read_excel('test.xlsx', dtype=dtypes, keep_default_na=False)
#train2= pd.read_excel('test_folds.xlsx', dtype=dtypes, keep_default_na=False)

print('train data: ', train.shape)
print('test data: ', test.shape)
#print(train2.shape)

train

test

train.head()
print(np.linspace(0,7660,11))

#shuffled =sample(frac=1)
#fold = np.array_split(train2, 10)
#fold0=pd.DataFrame(fold[0])
#fold1=pd.DataFrame(fold[1])
#fold2=pd.DataFrame(fold[2])
#fold3=pd.DataFrame(fold[3])
#fold4=pd.DataFrame(fold[4])
#fold5=pd.DataFrame(fold[5])
#fold6=pd.DataFrame(fold[6])
#fold7=pd.DataFrame(fold[7])
#fold8=pd.DataFrame(fold[8])
#fold9=pd.DataFrame(fold[9])

#fn= 9 bucati din 10
#f0=pd.concat([fold1,fold2,fold3,fold4,fold5,fold6,fold7,fold8,fold9])
#f1=pd.concat([fold0,fold2,fold3,fold4,fold5,fold6,fold7,fold8,fold9])
#f2=pd.concat([fold0,fold1,fold3,fold4,fold5,fold6,fold7,fold8,fold9])
#f3=pd.concat([fold0,fold1,fold2,fold4,fold5,fold6,fold7,fold8,fold9])
#f4=pd.concat([fold0,fold1,fold2,fold3,fold5,fold6,fold7,fold8,fold9])
#f5=pd.concat([fold0,fold1,fold2,fold3,fold4,fold6,fold7,fold8,fold9])
#f6=pd.concat([fold0,fold1,fold2,fold3,fold4,fold5,fold7,fold8,fold9])
#f7=pd.concat([fold0,fold1,fold2,fold3,fold4,fold5,fold6,fold8,fold9])
#f8=pd.concat([fold0,fold1,fold2,fold3,fold4,fold5,fold6,fold7,fold9])
#f9=pd.concat([fold0,fold1,fold2,fold3,fold4,fold5,fold6,fold7,fold8])

#numar de sinonime
def noSyn(word):
  synonyms = []

  for synset in wordnet.synsets(word):
    for l in synset.lemmas():
        synonyms.append(l.name())
  return(len(set(synonyms)))

#numar de antonime
def noAnt(word):

  antonyms = []
  for synset in wordnet.synsets(word):
    for l in synset.lemmas():
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())
  return(len(set(antonyms)))

def delete_punctuation(row):
  return row.translate(str.maketrans('', '', string.punctuation))

def position(row, word):
  roww=delete_punctuation(row)
  roww2 = roww.split(' ')
  pos=[-1]
  if word in roww2:
    pos = [roww2.index(word)]
  return pos

words=[]

def freq_in_folder(word):
  count=0
  for t in words:
    if t==word:
      count+=1    
  return count/len(words)

def freqlib(word):
  frq=zipf_frequency(word,'en')
  return [frq]

def frec(word):
  frec=word_frequency(word,'en')
  return frec

def square_brackets_Denisa(sentence):
    rez=re.search("\[.*?\]",sentence)
    if not rez:
        return [0]
    return [1]

def frequency_for_all_words(df):
    counterr=Counter()
    df.sentence.str.split().apply(counterr.update)
    return counterr

def frequency_for_a_word(word, df):
    counter_with_all_words=frequency_for_all_words(df)
    frequency_of_the_word=[counter_with_all_words[word]]
    return frequency_of_the_word

def corpus_feature(corpus):
    if corpus=="bible":
        return[0]
    elif corpus == 'biomed':
        return [1]
    else:
        return [2]

def length(word):
  return len(word)

def nr_vowels(word):
  c=0
  for i in word:
    if i in "aeiouAEIOU":
      c+=1
  return c

def nr_cons(word):
  return(length(word)-nr_vowels(word))

def is_dale_chall(word):
  return int(word.lower() in DALE_CHALL)

def is_title(word):
  return int(word.istitle())

def nr_syllables(word):
  langg=pyphen.Pyphen(lang='en')
  x=langg.inserted(word,'-').split('-')
  return len(x)

def get_all_tokens(df):
  all_words=[]
  for _,row in df.iterrows():
    #row['sentence'] =>Ana are mere
    tokens=word_tokenize(row['sentence'])
    #tokens=["Ana,are,mere"]
    for t in tokens:
      all_words.append(t)
  return all_words

def means(word):
  res=wordnet.synsets(word)
  return res

def nomeans(word):
  no=means(word)
  return len(no)

def hypermed(rez):
  hyper=0
  med=0
  if len(rez) > 0:
    for i in rez:
      hyper=hyper+len(i.hypernyms())
      med=hyper/len(rez)
  return med

def hypomed(rez):
  hyper=0
  med=0
  if len(rez) > 0:
    for i in rez:
      hyper=hyper+len(i.hyponyms())
    med=hyper/len(rez)
  return med

def mero1med(rez):
  mer=0
  med=0
  if len(rez)>0:
    for i in rez:
      mer=mer+len(i.part_meronyms())
    med=mer/(len(rez))
  return med

def mero2med(rez):
  mer=0
  med=0
  if len(rez)>0:
    for i in rez:
      mer=mer+len(i.substance_meronyms())
    med=mer/(len(rez))
  return med

def holo1med(rez):
  holo=0
  med=0
  if len(rez)>0:
    for i in rez:
      holo=holo+len(i.part_holonyms() )
    med=holo/(len(rez))
  return med

def holo2med(rez):
  holo=0
  med=0
  if len(rez)>0:
    for i in rez:
      holo=holo+len(i.substance_holonyms() )
    med=holo/(len(rez))
  return med

def etailmed(rez):
  et=0
  med=0
  if len(rez)>0:
    for i in rez:
      et=et+len(i.entailments() )
    med=et/(len(rez))
  return med

def synsets(word):
  return len(wordnet.synsets(word))

def def_length(word):
  bhkvl=0
  syns = wordnet.synsets(word)
  if len(syns) > 0:
    bhkvl=len(syns[0].definition())
  return bhkvl

def dist_fata_de_root(word):
  syns = wordnet.synsets(word)
  if len(syns)>0:
    root = syns[0].root_hypernyms()
    if len(root)>0:
      return root[0].path_similarity(syns[0])
  return 0

pos_words="CC CD DT EX FW IN JJ JJR JJS LS MD NN NNS NNP NNPS PDT POS PRP PRP$ RB RBR BS RP TO UH VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB"
pos_w_tokens=word_tokenize(pos_words)

punctuation="[-.,;:!?\"'/()_*=` ]"

punctuation2="[.,;:!?\"'/()_*=` ]"

def part_of_speach(word,row):
  val=posTagFunktion(word, row)
  de_returnat=-1
  ceva=-1
  if val:
    if val[0] in pos_w_tokens: # val[0] -- prima apritie in
      ceva=pos_w_tokens.index(val[0])
      de_returnat=ceva
  return de_returnat

def posTagFunktion(word, row):
  text = word_tokenize(row)
  x=nltk.pos_tag(text)
  x=np.array(x)
  ceva=[]
  for i in x:
    if word in i[0]:
      ceva.append(i[1])
  return ceva

def as_ci(row):
  let_range_1=range(65,90)
  let_range_2=range(97,122)
  alfa=0
  for i in row:
    if i not in punctuations:
      if (ord(i) not in let_range_1) and (ord(i) not in let_range_2):
        alfa+=1
  return [alfa]

"""Generarea de caracteristici legate de structura cuvantului pentru cuvantul tinta """

def get_word_structure_features(word):
    features = []
    features.append(nr_syllables(word))
    features.append(is_dale_chall(word))
    features.append(length(word))
    features.append(nr_vowels(word))
    features.append(nr_cons(word))
    #features.append(is_title(word))

    return np.array(features)

"""Generarea de caracteristici folosind Wordnet pentru cuvantul tinta"""

def get_wordnet_features(word):
  features = []
  features.append(synsets(word))
  #features.append(noSyn(word))
  #features.append(noAnt(word))
  word_means=means(word)
  #features.append(hypermed(word_means))
  #features.append(hypomed(word_means))
  #features.append(mero1med(word_means))
  #features.append(mero2med(word_means))
  #features.append(holo1med(word_means))
  #features.append(holo2med(word_means))
  #features.append(etailmed(word_means))
  features.append(dist_fata_de_root(word))
  features.append(def_length(word))
  return np.array(features)

"""Apelul functiilor de generare de caracteristici"""

def featurize(row):
    word = row['token']
    all_features = []
    all_features.extend(corpus_feature(row['corpus'])) # din ce categorie face parte
    all_features.extend(get_word_structure_features(word))
    all_features.extend(get_wordnet_features(word))
    all_features.extend(position(row['sentence'],word))
    all_features.extend(freqlib(word))
    #all_features.extend([part_of_speach(word,row['sentence'])])
    #all_features.extend(freq_in_folder(word))
    #all_features.extend(square_brackets_Denisa(str(row['sentence']))) #daca propozitia din care face parte are []
    #all_features.extend(as_ci(row['sentence'])) #caractere alfa numerice in propozitia din care face cuvantul
    return np.array(all_features)

def featurize_df(df):
    #words=get_all_tokens(df)
    nr_of_features = len(featurize(df.iloc[0]))
    nr_of_examples = len(df)
    features = np.zeros((nr_of_examples, nr_of_features))
    for index, row in df.iterrows():
        row_ftrs = featurize(row)
        features[index, :] = row_ftrs
    return features

"""Generarea de caracteristici pentru setul de train"""

X_train = featurize_df(train)
y_train = train['complex'].values

"""Generarea de caracteristici pentru setul de test"""

X_test = featurize_df(test)

# NAIVE BAYES
model = GaussianNB()
model.fit(X_train,y_train)
preds= model.predict(X_test)

test_id = np.arange(7663,9001)

np.savetxt("submisie_Kaggle_10proba.csv",np.stack((test_id,preds)).T,fmt="%d",delimiter=',',header="id,complex",comments="")